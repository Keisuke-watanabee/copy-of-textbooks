{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (1.15.5)\n",
      "Requirement already satisfied: h5py<=2.10.0 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.16.1)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.9.0)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "  Using cached tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (4.24.4)\n",
      "Collecting tensorboard<1.16.0,>=1.15.0\n",
      "  Using cached tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.2.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (46.4.0.post20200518)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 1.14.0\n",
      "    Uninstalling tensorflow-estimator-1.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 1.14.0\n",
      "    Uninstalling tensorboard-1.14.0:\n",
      "      Successfully uninstalled tensorboard-1.14.0\n",
      "Successfully installed tensorboard-1.15.0 tensorflow-estimator-1.15.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages (1.18.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"tensorflow>=1.15,<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"tensorflow-gpu>=1.15,<2.0\" (If GPU support needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras (installed with latest 2.4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==2.2.4 (downgraded to 2.2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user --upgrade tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user --upgrade tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2fdbad3ab9ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"random_uniform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Dense' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer=\"random_uniform\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1671) # for reproducibility\n",
    "#network and training\n",
    "nb_epoch = 200\n",
    "batch_size = 128\n",
    "verbose =1\n",
    "nb_classes = 10 #number of outputs = number of digits\n",
    "optimaizer = SGD()  #SGDoptimizer, explained reserved for validation\n",
    "n_hidden = 128\n",
    "validation_split = 0.2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを訓練データとテストデータにシャッフルする\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_train is 60000raw of 28*28 values >>>> resahped in 60000 * 784\n",
    "reshaped = 784\n",
    "x_train = x_train.reshape(60000, reshaped)\n",
    "x_test = x_test.reshape(10000, reshaped)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "#標準化\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10outputs\n",
    "#final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(nb_classes, input_shape=(reshaped,)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import gmtime, strftime\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "def make_tensorboard(set_dir_name=\"\"):\n",
    "    tictoc = strftime(\"%a_%d_%b_%y_%h_%m_%s\", gmtime())\n",
    "    directory_name = tictoc\n",
    "    log_dir = set_dir_name + \"_\" + directory_name\n",
    "    os.mkdir(log_dir)\n",
    "    tensorboard = TensorBoard(log_dir = log_dir)\n",
    "    return tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#callbacks = [keras.callbacks.TensorBoard(log_dir=\"/Users/watanabekeisuke/Desktop/tflog/\", histogram_freq=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [make_tensorboard(set_dir_name=\"keras_MNISTV1\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimaizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch, callbacks = callbacks, verbose=verbose, \n",
    "         validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "print(\"test score :\", score[0])\n",
    "print(\"test accuracy :\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard --logdir= /Users/watanabekeisuke/Desktop/tflog/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "#network and training\n",
    "nb_epoch = 20\n",
    "batch_size = 128\n",
    "verbose =1\n",
    "nb_classes = 10 #number of outputs = number of digits\n",
    "optimaizer = SGD()  #SGDoptimizer, explained reserved for validation\n",
    "n_hidden = 128\n",
    "validation_split = 0.2\n",
    "\n",
    "#データを訓練データとテストデータにシャッフルする\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_train is 60000raw of 28*28 values >>>> resahped in 60000 * 784\n",
    "reshaped = 784\n",
    "x_train = x_train.reshape(60000, reshaped)\n",
    "x_test = x_test.reshape(10000, reshaped)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "#標準化\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "#M_HIDDEN hidden layer\n",
    "#10outputs\n",
    "#final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_shape=(reshaped,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimaizer, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [make_tensorboard(set_dir_name=\"keras_MNISTV2\")]\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch, callbacks = callbacks, verbose=verbose, \n",
    "         validation_split=validation_split)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "print(\"test score :\", score[0])\n",
    "print(\"test accuracy :\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropoutを適用\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "#network and training\n",
    "nb_epoch = 20\n",
    "batch_size = 128\n",
    "verbose =1\n",
    "nb_classes = 10 #number of outputs = number of digits\n",
    "optimaizer = SGD()  #SGDoptimizer, explained reserved for validation\n",
    "n_hidden = 128\n",
    "validation_split = 0.2\n",
    "drop_out = 0.3\n",
    "\n",
    "#データを訓練データとテストデータにシャッフルする\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_train is 60000raw of 28*28 values >>>> resahped in 60000 * 784\n",
    "reshaped = 784\n",
    "x_train = x_train.reshape(60000, reshaped)\n",
    "x_test = x_test.reshape(10000, reshaped)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "#標準化\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "#M_HIDDEN hidden layer\n",
    "#10outputs\n",
    "#final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_shape=(reshaped,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimaizer, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [make_tensorboard(set_dir_name=\"keras_MNISTV2\")]\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch, callbacks = callbacks, verbose=verbose, \n",
    "         validation_split=validation_split)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "print(\"test score :\", score[0])\n",
    "print(\"test accuracy :\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizerにRMDpropを採用\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "#network and training\n",
    "nb_epoch = 20\n",
    "batch_size = 128\n",
    "verbose =1\n",
    "nb_classes = 10 #number of outputs = number of digits\n",
    "optimaizer = RMSprop()  #SGDoptimizer, explained reserved for validation\n",
    "n_hidden = 128\n",
    "validation_split = 0.2\n",
    "drop_out = 0.3\n",
    "\n",
    "#データを訓練データとテストデータにシャッフルする\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_train is 60000raw of 28*28 values >>>> resahped in 60000 * 784\n",
    "reshaped = 784\n",
    "x_train = x_train.reshape(60000, reshaped)\n",
    "x_test = x_test.reshape(10000, reshaped)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "#標準化\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "#M_HIDDEN hidden layer\n",
    "#10outputs\n",
    "#final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_shape=(reshaped,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimaizer, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [make_tensorboard(set_dir_name=\"keras_MNISTV2\")]\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch, callbacks = callbacks, verbose=verbose, \n",
    "         validation_split=validation_split)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "print(\"test score :\", score[0])\n",
    "print(\"test accuracy :\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizerにAdamを採用\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "#network and training\n",
    "nb_epoch = 5\n",
    "batch_size = 128\n",
    "verbose =1\n",
    "nb_classes = 10 #number of outputs = number of digits\n",
    "optimaizer = Adam()  #SGDoptimizer, explained reserved for validation\n",
    "n_hidden = 128\n",
    "validation_split = 0.2\n",
    "drop_out = 0.3\n",
    "\n",
    "#データを訓練データとテストデータにシャッフルする\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#x_train is 60000raw of 28*28 values >>>> resahped in 60000 * 784\n",
    "reshaped = 784\n",
    "x_train = x_train.reshape(60000, reshaped)\n",
    "x_test = x_test.reshape(10000, reshaped)\n",
    "x_train = x_train.astype(\"float32\")\n",
    "x_test = x_test.astype(\"float32\")\n",
    "\n",
    "#標準化\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "#convert class vectors to binary class matrices\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "#M_HIDDEN hidden layer\n",
    "#10outputs\n",
    "#final stage is softmax\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden, input_shape=(reshaped,)))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(drop_out))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimaizer, metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [make_tensorboard(set_dir_name=\"keras_MNISTV2\")]\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=nb_epoch, callbacks = callbacks, verbose=verbose, \n",
    "         validation_split=validation_split)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=verbose)\n",
    "\n",
    "print(\"test score :\", score[0])\n",
    "print(\"test accuracy :\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EarlyStoppingコールバック。。。評価関数の値が改善されなくなったら時に学習を停止できる\n",
    "keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=0, verbose=0, mode=\"auto\")\n",
    "\n",
    "#損失履歴の保存\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get(\"loss\"))\n",
    "        \n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=784, init=\"uniform\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
    "history = LossHistory()\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=20, verbose=0, callbacks=[history])\n",
    "print(history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenet(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        20, kernel_size=5, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(50, kernel_size=5, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(500, activation=\"relu\"))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (28, 28, 1)  # image is 28x28x1 (grayscale)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_lenet\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(\"/Users/watanabekeisuke/Desktop/tflog\"), logdir)\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
    "            verbose=self.verbose\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2503: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:783: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:786: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/12\n",
      "48000/48000 [==============================] - 48s 1ms/step - loss: 0.1795 - acc: 0.9445 - val_loss: 0.0553 - val_acc: 0.9840\n",
      "WARNING:tensorflow:From /Users/watanabekeisuke/anaconda3/lib/python3.7/site-packages/keras/callbacks.py:869: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/12\n",
      "48000/48000 [==============================] - 301s 6ms/step - loss: 0.0456 - acc: 0.9857 - val_loss: 0.0483 - val_acc: 0.9872\n",
      "Epoch 3/12\n",
      "48000/48000 [==============================] - 59s 1ms/step - loss: 0.0321 - acc: 0.9898 - val_loss: 0.0579 - val_acc: 0.9834\n",
      "Epoch 4/12\n",
      "48000/48000 [==============================] - 52s 1ms/step - loss: 0.0231 - acc: 0.9923 - val_loss: 0.0361 - val_acc: 0.9897\n",
      "Epoch 5/12\n",
      "48000/48000 [==============================] - 48s 1ms/step - loss: 0.0185 - acc: 0.9940 - val_loss: 0.0376 - val_acc: 0.9887\n",
      "Epoch 6/12\n",
      "48000/48000 [==============================] - 49s 1ms/step - loss: 0.0129 - acc: 0.9957 - val_loss: 0.0320 - val_acc: 0.9907\n",
      "Epoch 7/12\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.0098 - acc: 0.9967 - val_loss: 0.0341 - val_acc: 0.9906\n",
      "Epoch 8/12\n",
      "48000/48000 [==============================] - 66s 1ms/step - loss: 0.0085 - acc: 0.9971 - val_loss: 0.0426 - val_acc: 0.9886\n",
      "Epoch 9/12\n",
      "48000/48000 [==============================] - 51s 1ms/step - loss: 0.0084 - acc: 0.9971 - val_loss: 0.0434 - val_acc: 0.9898\n",
      "Epoch 10/12\n",
      "48000/48000 [==============================] - 49s 1ms/step - loss: 0.0087 - acc: 0.9972 - val_loss: 0.0420 - val_acc: 0.9895\n",
      "Epoch 11/12\n",
      "48000/48000 [==============================] - 49s 1ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0356 - val_acc: 0.9909\n",
      "Epoch 12/12\n",
      "48000/48000 [==============================] - 47s 982us/step - loss: 0.0063 - acc: 0.9979 - val_loss: 0.0441 - val_acc: 0.9893\n",
      "Test loss: 0.03555262182720007\n",
      "Test accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "dataset = MNISTDataset()\n",
    "\n",
    "# make model\n",
    "model = lenet(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=Adam())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    "    )\n",
    "\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 1.7055 - acc: 0.3891 - val_loss: 1.3921 - val_acc: 0.5269\n",
      "Epoch 2/12\n",
      "40000/40000 [==============================] - 43s 1ms/step - loss: 1.3134 - acc: 0.5349 - val_loss: 1.2470 - val_acc: 0.5620\n",
      "Epoch 3/12\n",
      "40000/40000 [==============================] - 43s 1ms/step - loss: 1.1424 - acc: 0.5981 - val_loss: 1.0602 - val_acc: 0.6306\n",
      "Epoch 4/12\n",
      "40000/40000 [==============================] - 43s 1ms/step - loss: 1.0326 - acc: 0.6389 - val_loss: 0.9551 - val_acc: 0.6735\n",
      "Epoch 5/12\n",
      "40000/40000 [==============================] - 43s 1ms/step - loss: 0.9493 - acc: 0.6667 - val_loss: 1.0706 - val_acc: 0.6364\n",
      "Epoch 6/12\n",
      "40000/40000 [==============================] - 43s 1ms/step - loss: 0.8808 - acc: 0.6930 - val_loss: 0.8836 - val_acc: 0.6984\n",
      "Epoch 7/12\n",
      "40000/40000 [==============================] - 43s 1ms/step - loss: 0.8205 - acc: 0.7130 - val_loss: 0.9355 - val_acc: 0.6837\n",
      "Epoch 8/12\n",
      "40000/40000 [==============================] - 44s 1ms/step - loss: 0.7649 - acc: 0.7342 - val_loss: 0.8623 - val_acc: 0.7057\n",
      "Epoch 9/12\n",
      "40000/40000 [==============================] - 44s 1ms/step - loss: 0.7187 - acc: 0.7472 - val_loss: 0.8697 - val_acc: 0.7043\n",
      "Epoch 10/12\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 0.6791 - acc: 0.7642 - val_loss: 0.8235 - val_acc: 0.7207\n",
      "Epoch 11/12\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 0.6334 - acc: 0.7807 - val_loss: 0.9331 - val_acc: 0.6842\n",
      "Epoch 12/12\n",
      "40000/40000 [==============================] - 45s 1ms/step - loss: 0.6046 - acc: 0.7894 - val_loss: 0.8107 - val_acc: 0.7296\n",
      "Test loss: 0.8257313828468322\n",
      "Test accuracy: 0.7251\n"
     ]
    }
   ],
   "source": [
    "#CIFAR-10の画像分類\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        32, kernel_size=3, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (32, 32, 3)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_net\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(\"/Users/watanabekeisuke/Desktop/tflog\"), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "dataset = CIFAR10Dataset()\n",
    "\n",
    "# make model\n",
    "model = network(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    "    )\n",
    "\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#CIFAR-10の画像分類をもっと深く\n",
    "\n",
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "\n",
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    # extract image features by convolution and max pooling layers\n",
    "    model.add(Conv2D(\n",
    "        32, kernel_size=3, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # classify the class by fully-connected layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model\n",
    "\n",
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (32, 32, 3)\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            # convert class vectors to binary class matrices\n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  # convert the value to 0~1 scale\n",
    "            shape = (data.shape[0],) + self.image_shape  # add dataset length\n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_net\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(\"/Users/watanabekeisuke/Desktop/tflog\"), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)  # remove previous execution\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose\n",
    "        )\n",
    "\n",
    "\n",
    "dataset = CIFAR10Dataset()\n",
    "\n",
    "# make model\n",
    "model = network(dataset.image_shape, dataset.num_classes)\n",
    "\n",
    "# train the model\n",
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    "    )\n",
    "\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
